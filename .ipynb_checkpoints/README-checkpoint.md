# Tracking Representation Drift in Transformers via Optimal Transport

## Project Overview

This project studies how internal representations evolve across transformer layers and training epochs using Optimal Transport (OT). We track the "drift" of information through transformer models during training to detect potential emergent behaviors and phase transitions.

### **Primary Goal: Geometric Fingerprint Analysis**

The core objective is to analyze the **"geometric fingerprint"** that distinguishes trained models from untrained (randomly initialized) models. This reveals how learning creates meaningful representation structure and enables detection of emergent behaviors.

### What We're Doing

1. **Train on GSM8K**: Use GPT-2 style transformers on mathematical reasoning dataset
2. **Extract Activations**: Get hidden state representations from each layer at different training checkpoints
3. **Compute OT Distances**: Use Sinkhorn distances to measure representation changes across layers and time
4. **Visualize Drift**: Plot how representations compress, drift, or restructure during training
5. **Detect Emergence**: Look for phase transitions and sudden changes in representation structure
6. **Compare Trained vs Untrained**: Analyze geometric transformation paths between learned and random models

### Key Concepts

- **Optimal Transport**: Mathematical framework for measuring the "cost" of transforming one distribution into another
- **Representation Drift**: How internal representations change during training
- **Phase Transitions**: Sudden qualitative changes in model behavior during training
- **Sinkhorn Distance**: Regularized OT method for efficient computation
- **Geometric Fingerprint**: Unique transformation patterns that distinguish trained from untrained models

## üéØ **Key Feature: Geometric Fingerprint Analysis**

This project includes a crucial comparison between **trained and untrained models** to analyze the "fingerprint" of learned structure:

### **What This Reveals:**
- **Trained Model**: Shows organized, structured geometric transformations
- **Untrained Model**: Shows random, chaotic transformation patterns
- **Comparison**: Reveals how learning creates meaningful representation structure

### **Research Insights:**
- **Smoother Paths**: Trained models show smoother geometric transformations
- **Structured Compression**: Learned models exhibit systematic information compression
- **Emergent Organization**: Training creates coherent representation structure

## Setup

```bash
pip install -r requirements.txt
```

## üöÄ **Main Scripts to Run**

### **Primary Experiment: Trained vs Untrained Comparison**
```bash
# Run the core geometric fingerprint analysis
python trained_vs_untrained_comparison.py
```

### **Training Dynamics Tracking**
```bash
# Track representation changes during training
python train_and_track.py
```

## üìä **Module Explanations**

### **Core Analysis Modules:**

#### **`trained_vs_untrained_comparison.py`** - **Primary Script**
- **Purpose**: Compare geometric transformation paths between trained and untrained models
- **What it does**: 
  - Loads pre-trained GPT-2 Large model
  - Creates randomly initialized model with same architecture
  - Extracts activations from both models on GSM8K data
  - Computes OT distances between adjacent layers
  - Analyzes entropy and transformation patterns
  - Generates comparison visualizations
- **Output**: Geometric fingerprint analysis, entropy comparison, transformation type analysis

#### **`train_and_track.py`** - **Training Dynamics**
- **Purpose**: Track representation evolution during training
- **What it does**:
  - Trains GPT-2 model on GSM8K dataset
  - Saves checkpoints at regular intervals
  - Extracts activations at each checkpoint
  - Analyzes representation drift across training
  - Detects phase transitions and learning shifts
- **Output**: Training dynamics, checkpoint activations, drift analysis

### **Supporting Modules:**

#### **`transformer_utils.py`**
- **Purpose**: Extract hidden state activations from transformer layers
- **Key Functions**:
  - `ActivationExtractor`: Extract activations from all layers
  - `TrainingTracker`: Track training progress and save checkpoints
  - `ModelComparison`: Compare trained vs untrained models

#### **`ot_analysis.py`**
- **Purpose**: Compute Optimal Transport distances and analyze transformations
- **Key Functions**:
  - `OTAnalyzer`: Main OT computation engine
  - `compute_ot_distance`: Calculate Sinkhorn distances
  - `analyze_representation_flow`: Analyze layer-wise transformations
  - `detect_phase_transitions`: Identify sudden changes

#### **`data_utils.py`**
- **Purpose**: Load and process GSM8K dataset
- **Key Functions**:
  - `GSM8KDataset`: Dataset loader for math word problems
  - `create_dataloader`: Create training batches
  - `get_sample_data`: Extract sample questions for analysis

#### **`visualization.py`**
- **Purpose**: Create comprehensive visualizations
- **Key Functions**:
  - `OTVisualizer`: Main visualization engine
  - `plot_ot_distances`: Plot layer-wise OT distances
  - `create_comprehensive_dashboard`: Generate all plots

## üìà **Current Results & Analysis**

### **Latest Experimental Results:**

#### **Geometric Fingerprint Analysis (GPT-2 Large, 3000 samples):**
```
OT Distance Comparison:
  Trained model mean distance: 0.8372
  Untrained model mean distance: 1.1873
  Mean difference (trained - untrained): -0.3502
  Layer pair with biggest difference: L13-L14 (-0.6901)

Entropy Comparison:
  Trained model mean entropy: 11.7447
  Untrained model mean entropy: 12.0873
  Mean entropy difference: -0.3426
```

#### **Key Findings:**
- **‚úÖ Geometric Fingerprint Detected**: Clear distinction between trained and untrained models
- **‚úÖ Structured Compression**: Trained model shows 29% lower OT distances
- **‚úÖ Information Organization**: Trained model has 2.8% lower entropy
- **‚úÖ Layer-Specific Effects**: L13-L14 shows strongest learning signature
- **‚úÖ Emergent Structure**: Training creates coherent representation organization

### **Training Dynamics Results (15 epochs, 1000 samples):**
- **Loss Reduction**: 9.6 ‚Üí 2.36 (75% improvement)
- **Convergence**: Stable learning with no overfitting
- **Representation Drift**: Systematic changes across training epochs
- **Phase Transitions**: Detected at key learning milestones

## ‚öôÔ∏è **Current Configuration**

### **Trained vs Untrained Comparison:**
```python
config = {
    'model_name': 'gpt2-large',      # 774M parameters
    'num_samples': 3000,             # Comprehensive dataset
    'max_length': 512,               # Full sequence analysis
    'ot_method': 'sinkhorn',         # Fast, regularized OT
    'pca_components': 50,            # Dimensionality reduction
    'output_dir': 'trained_vs_untrained_results'
}
```

### **Training Dynamics:**
```python
config = {
    'model_name': 'gpt2-large',      # 774M parameters
    'num_epochs': 15,                # Complete training cycle
    'batch_size': 2,                 # Memory-efficient
    'learning_rate': 1e-6,           # Stable learning
    'max_length': 512,               # Full sequences
    'num_samples': 1000,             # Good statistical power
    'checkpoint_frequency': 999999,  # Disable checkpoints
    'output_dir': 'training_results'
}
```

## üìÅ **Results & Data Storage**

### **Available Results:**
- **‚úÖ Geometric Fingerprint Analysis**: `trained_vs_untrained_results/`
- **‚úÖ Training Dynamics**: `training_results/`
- **‚úÖ Visualizations**: PNG plots and heatmaps
- **‚úÖ Configuration Files**: JSON configs for reproducibility

### **Checkpoint Data (Large Files):**
- **Training Checkpoints**: `training_results/checkpoints/` (45GB total) [Link to be added]
- **Model States**: Full model checkpoints at each epoch
- **Activation Files**: Layer-wise activations for analysis
- **Storage**: Available on Google Drive
- 
## üöÄ Lambda.ai Deployment

### **Yes, you can run this on Lambda.ai!**

This project is fully compatible with Lambda.ai GPU instances. The setup uses:
- **PyTorch** with CUDA support
- **HuggingFace Transformers** for model loading
- **POT** (Python Optimal Transport) for OT computations
- **GSM8K dataset** (automatically downloaded via HuggingFace)

### **Lambda.ai Setup Steps:**

1. **Launch GPU Instance**: Choose a GPU instance (A10, A100, or H100)
2. **Install Dependencies**: 
   ```bash
   pip install -r requirements.txt
   ```
3. **Run Experiments**: 
   ```bash
   python trained_vs_untrained_comparison.py  # Geometric fingerprint
   python train_and_track.py                  # Training dynamics
   ```

### **Recommended Lambda.ai Configurations:**

| Use Case | Instance Type | Memory | GPU | Estimated Cost | Time |
|----------|---------------|---------|-----|----------------|------|
| **Quick Testing** | GPU 1x A10 | 24GB | 1x A10 | ~$0.60/hour | 30-60 min |
| **Medium Research** | GPU 1x A100 | 80GB | 1x A100 | ~$2.40/hour | 15-30 min |
| **Large Research** | GPU 8x A100 | 640GB | 8x A100 | ~$19.20/hour | 5-10 min |

## üî¨ Serious Research Setup

### **Parameter Tuning Guide**

#### **Model Size Selection:**

```python
# Quick Testing (CPU/Laptop)
config = {
    'model_name': 'gpt2',  # 124M params
    'num_samples': 20,
    'batch_size': 2,
    'max_length': 128
}

# Medium Research (Single GPU)
config = {
    'model_name': 'gpt2-medium',  # 355M params
    'num_samples': 100,
    'batch_size': 4,
    'max_length': 256
}

# Serious Research (Multi-GPU)
config = {
    'model_name': 'gpt2-large',  # 774M params
    'num_samples': 500,
    'batch_size': 2,
    'max_length': 512
}

# Large Scale Research (High-end GPU)
config = {
    'model_name': 'gpt2-xl',  # 1.5B params
    'num_samples': 1000,
    'batch_size': 1,
    'max_length': 512
}
```

#### **Computational Requirements:**

| Model | Parameters | VRAM | Training Time | Use Case |
|-------|------------|------|---------------|----------|
| **GPT-2** | 124M | 2-4GB | 5-10 min | Quick testing |
| **GPT-2 Medium** | 355M | 4-8GB | 15-30 min | Medium research |
| **GPT-2 Large** | 774M | 8-16GB | 30-60 min | Serious research |
| **GPT-2 XL** | 1.5B | 16-32GB | 60-120 min | Large scale |

#### **Dataset Access:**

The GSM8K dataset is **automatically downloaded** when you run the code:

```python
# In data_utils.py - happens automatically
from datasets import load_dataset
dataset = load_dataset("gsm8k", "main")
```

**Dataset Details:**
- **Size**: ~8K grade school math problems
- **Format**: Question + step-by-step solution
- **Download**: ~50MB (cached locally)
- **Access**: No API key required

#### **Training Configuration Tuning:**

```python
# For Lambda.ai GPU instances
config = {
    'model_name': 'gpt2-large',  # Choose based on VRAM
    'num_epochs': 3,  # Increase for better results
    'batch_size': 1,  # Reduce if OOM, increase if VRAM available
    'learning_rate': 5e-5,  # Standard for fine-tuning
    'max_length': 512,  # Increase for longer sequences
    'num_samples': 200,  # Increase for better statistics
    'checkpoint_frequency': 20,  # More frequent = more data points
    'output_dir': 'results'
}
```

### **Performance Optimization:**

#### **Memory Management:**
```python
# Reduce memory usage
config = {
    'batch_size': 1,  # Smallest batch size
    'max_length': 256,  # Shorter sequences
    'pca_components': 20,  # Fewer PCA components
    'num_samples': 50  # Fewer samples
}
```

#### **Speed Optimization:**
```python
# Faster computation
config = {
    'ot_method': 'sinkhorn',  # Faster than EMD
    'pca_components': 30,  # Balance speed vs accuracy
    'checkpoint_frequency': 50  # Fewer checkpoints
}
```

### **Lambda.ai Specific Notes:**

#### **Instance Selection:**
- **A10 (24GB VRAM)**: Perfect for GPT-2 Medium research
- **A100 (80GB VRAM)**: Ideal for GPT-2 Large/XL research
- **H100 (80GB VRAM)**: Best for large-scale experiments

#### **Cost Optimization:**
- **Spot Instances**: 60-80% cost savings
- **Auto-shutdown**: Set up to stop when training completes
- **Data Persistence**: Use Lambda.ai's persistent storage

#### **Monitoring:**
```bash
# Monitor GPU usage
nvidia-smi

# Monitor memory usage
htop

# Check training progress
tail -f training_results/results.json
```

### **Advanced Research Extensions:**

#### **Multi-GPU Training:**
```python
# For 8x A100 setup
config = {
    'model_name': 'gpt2-xl',
    'num_samples': 2000,
    'batch_size': 8,  # Can increase with more GPUs
    'use_multi_gpu': True
}
```

#### **Long Training Runs:**
```python
# For extended research
config = {
    'num_epochs': 10,
    'num_samples': 1000,
    'checkpoint_frequency': 10,
    'save_final_model': True
}
```

#### **Hyperparameter Search:**
```python
# Grid search over parameters
learning_rates = [1e-5, 5e-5, 1e-4]
batch_sizes = [1, 2, 4]
model_sizes = ['gpt2-medium', 'gpt2-large']

for lr in learning_rates:
    for bs in batch_sizes:
        for model in model_sizes:
            config = {
                'model_name': model,
                'learning_rate': lr,
                'batch_size': bs,
                'output_dir': f'results_{model}_lr{lr}_bs{bs}'
            }
            # Run experiment
```

### **Expected Results:**

#### **Quick Test (5-10 minutes):**
- Basic OT distance patterns
- Layer-wise representation evolution
- Simple visualizations

#### **Medium Research (30-60 minutes):**
- Training dynamics tracking
- Phase transition detection
- Comprehensive drift analysis

#### **Serious Research (2-4 hours):**
- Multi-epoch training analysis
- Statistical significance testing
- Publication-ready visualizations

### **Troubleshooting:**

#### **Out of Memory (OOM):**
```python
# Reduce these parameters
config = {
    'batch_size': 1,  # Most important
    'max_length': 128,  # Reduce sequence length
    'num_samples': 20,  # Fewer samples
    'pca_components': 10  # Fewer components
}
```

#### **Slow Training:**
```python
# Speed up with these changes
config = {
    'ot_method': 'sinkhorn',  # Faster than EMD
    'checkpoint_frequency': 100,  # Fewer checkpoints
    'pca_components': 20  # Balance speed/accuracy
}
```

#### **Dataset Download Issues:**
```bash
# Manual dataset download
python -c "from datasets import load_dataset; load_dataset('gsm8k', 'main')"
```

## üìä **Research Significance**

### **Scientific Contributions:**
1. **Geometric Fingerprint Discovery**: First systematic comparison of trained vs untrained geometric transformations
2. **Emergence Detection**: Novel method for detecting emergent behaviors in transformers
3. **Representation Drift Analysis**: Comprehensive tracking of representation evolution
4. **Phase Transition Identification**: Automated detection of learning milestones

### **Practical Applications:**
1. **Model Interpretability**: Understanding how transformers learn
2. **Training Monitoring**: Real-time analysis of learning progress
3. **Architecture Design**: Insights for better model architectures
4. **AI Safety**: Detection of unexpected behaviors during training

This setup provides a complete research framework that scales from quick testing to serious research, with full Lambda.ai compatibility and comprehensive parameter tuning guidelines. 